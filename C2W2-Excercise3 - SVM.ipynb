{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "This notebook is designed to run in a IBM Watson Studio default runtime (NOT the Watson Studio Apache Spark Runtime as the default runtime with 1 vCPU). Therefore,  Apache Spark is installed in local mode (for test purposes only). We wont use it in production.\n\nIn case of facing issues, the following two documents could help me:\n\nhttps://github.com/IBM/skillsnetwork/wiki/Environment-Setup\n\nhttps://github.com/IBM/skillsnetwork/wiki/FAQ\n\nThen, I can ask for help here:\n\nhttps://coursera.org/learn/machine-learning-big-data-apache-spark/discussions/all\n\nBefore asking a question, I should  make sure to follow the guidelines:\n\nhttps://github.com/IBM/skillsnetwork/wiki/FAQ#im-feeling-lost-and-confused-please-help-me\n\n\nIf running outside Watson Studio, this should work as well. In case I am running in an Apache Spark context outside Watson Studio, I must remove the Apache Spark setup in the first notebook cells."}, {"metadata": {}, "cell_type": "code", "source": "from IPython.display import Markdown, display\ndef printmd(string):\n    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n\n\nif ('sc' in locals() or 'sc' in globals()):\n    printmd('<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>')\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!pip install pyspark==2.4.5", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "try:\n    from pyspark import SparkContext, SparkConf\n    from pyspark.sql import SparkSession\nexcept ImportError as e:\n    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "\n\nIn case you want to learn how ETL is done, please run the following notebook first and update the file name below accordingly\n\nhttps://github.com/IBM/coursera/blob/master/coursera_ml/a2_w1_s3_ETL.ipynb\n"}, {"metadata": {}, "cell_type": "code", "source": "# delete files from previous runs\n!rm -f hmp.parquet*\n\n# download the file containing the data in PARQUET format\n!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n    \n# create a dataframe out of it\ndf = spark.read.parquet('hmp.parquet')\n\n# register a corresponding query table\ndf.createOrReplaceTempView('df')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "splits = df.randomSplit([0.8, 0.2])\ndf_train = splits[0]\ndf_test = splits[1]", "execution_count": 8, "outputs": []}, {"metadata": {"pixiedust": {"displayParams": {}}}, "cell_type": "code", "source": "from pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import MinMaxScaler\n\nindexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\nencoder = OneHotEncoder(inputCol=\"label\", outputCol=\"labelVec\")\nvectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n                                  outputCol=\"features\")\nnormalizer = MinMaxScaler(inputCol=\"features\", outputCol=\"features_norm\")\n\n", "execution_count": 9, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.ml.classification import LinearSVC\n\nlsvc = LinearSVC(maxIter=10, regParam=0.1)", "execution_count": 10, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df.createOrReplaceTempView('df')\ndf_two_class = spark.sql(\"select * from df where class in ('Use_telephone','Standup_chair')\")\n", "execution_count": 11, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "splits = df_two_class.randomSplit([0.8, 0.2])\ndf_train = splits[0]\ndf_test = splits[1]", "execution_count": 12, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.ml import Pipeline\npipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer,lsvc])\n", "execution_count": 13, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "model = pipeline.fit(df_train)\n\n", "execution_count": 14, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "prediction = model.transform(df_train)", "execution_count": 15, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(prediction)", "execution_count": 16, "outputs": [{"data": {"text/plain": "0.939600550560836"}, "execution_count": 16, "metadata": {}, "output_type": "execute_result"}]}, {"metadata": {}, "cell_type": "code", "source": "prediction = model.transform(df_test)", "execution_count": 17, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(prediction)", "execution_count": 18, "outputs": [{"data": {"text/plain": "0.9357356471202974"}, "execution_count": 18, "metadata": {}, "output_type": "execute_result"}]}], "metadata": {"kernelspec": {"name": "python36", "display_name": "Python 3.6 with Spark", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.8", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4, "nbformat_minor": 4}
